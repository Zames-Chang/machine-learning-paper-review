{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\n",
    "exercise = 2  # Possible values: 1, 2, 3, or 4.\n",
    "\n",
    "from datasets import generate_x_y_data_v1, generate_x_y_data_v2, generate_x_y_data_v3, generate_x_y_data_v4 \n",
    "\n",
    "# We choose which data function to use below, in function of the exericse. \n",
    "if exercise == 1:\n",
    "    generate_x_y_data = generate_x_y_data_v1\n",
    "if exercise == 2:\n",
    "    generate_x_y_data = generate_x_y_data_v2\n",
    "if exercise == 3:\n",
    "    generate_x_y_data = generate_x_y_data_v3\n",
    "if exercise == 4:  \n",
    "    generate_x_y_data = generate_x_y_data_v4\n",
    "import tensorflow as tf  # Version 1.0 or 0.12\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_x,sample_y = generate_x_y_data(isTrain=True, batch_size=3)\n",
    "\n",
    "# Backward compatibility for TensorFlow's version 0.12: \n",
    "try:\n",
    "    tf.nn.seq2seq = tf.contrib.legacy_seq2seq\n",
    "    tf.nn.rnn_cell = tf.contrib.rnn\n",
    "    tf.nn.rnn_cell.GRUCell = tf.contrib.rnn.GRUCell\n",
    "    print(\"TensorFlow's version : 1.0 (or more)\")\n",
    "except: \n",
    "    print(\"TensorFlow's version : 0.12\")\n",
    "seq_length = sample_x.shape[0]  # Time series will have the same past and future (to be predicted) lenght. \n",
    "batch_size = 50  # Low value used for live demo purposes - 100 and 1000 would be possible too, crank that up!\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "有關超參數的調整在下面可以做調整\n",
    "\"\"\"\n",
    "output_dim = sample_y.shape[-1]\n",
    "input_dim = sample_x.shape[-1]  # Output dimension (e.g.: multiple signals at once, tied in time)\n",
    "hidden_dim = 35  # Count of hidden neurons in the recurrent units. \n",
    "layers_stacked_count = 2  # Number of stacked recurrent cells, on the neural depth axis. \n",
    "\n",
    "# Optmizer: \n",
    "learning_rate = 0.007  # Small lr helps not to diverge during training. \n",
    "nb_iters = 2500  # How many times we perform a training step (therefore how many times we show a batch). \n",
    "lr_decay = 0.92  # default: 0.9 . Simulated annealing.\n",
    "momentum = 0.5  # default: 0.0 . Momentum technique in weights update\n",
    "lambda_l2_reg = 0.003  # L2 regularization of weights - avoids overfitting\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('biRNN'):\n",
    "    inp = [\n",
    "        tf.placeholder(tf.float32, shape=(None, input_dim), name=\"inp_{}\".format(t))\n",
    "           for t in range(seq_length)\n",
    "    ]\n",
    "    expected_sparse_output = [\n",
    "        tf.placeholder(tf.float32, shape=(None, output_dim), name=\"expected_sparse_output_\".format(t))\n",
    "          for t in range(seq_length)\n",
    "    ]\n",
    "    dec_inp = [ tf.zeros_like(enc_inp[0], dtype=np.float32, name=\"GO\") ] + enc_inp[:-1]\n",
    "    cells = []\n",
    "    for i in range(layers_stacked_count):\n",
    "        with tf.variable_scope('RNN_{}'.format(i)):\n",
    "            cells.append(tf.nn.rnn_cell.GRUCell(hidden_dim))\n",
    "            # cells.append(tf.nn.rnn_cell.BasicLSTMCell(...))\n",
    "    cell = tf.nn.rnn_cell.MultiRNNCell(cells)\n",
    "    dec_outputs, dec_memory = tf.nn.seq2seq.basic_rnn_seq2seq(\n",
    "        enc_inp, \n",
    "        dec_inp, \n",
    "        cell\n",
    "    )\n",
    "    w_out = tf.Variable(tf.random_normal([hidden_dim, output_dim]))\n",
    "    b_out = tf.Variable(tf.random_normal([output_dim]))\n",
    "    output_scale_factor = tf.Variable(1.0, name=\"Output_ScaleFactor\")\n",
    "    reshaped_outputs = [output_scale_factor*(tf.matmul(i, w_out) + b_out) for i in dec_outputs]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
