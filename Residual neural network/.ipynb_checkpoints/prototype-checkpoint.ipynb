{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-6130171e0e00>:4: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\Users\\User\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\Users\\User\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\User\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\User\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\User\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual_net(object):\n",
    "    def __init__(self,input_size,learning_rate = 0.001):\n",
    "        self.input_size = input_size\n",
    "        self.image_width = input_size[0]\n",
    "        self.image_height = input_size[1]\n",
    "        self.num_classes = 10\n",
    "        self.input_x = tf.placeholder(\"float\",name='input',shape = [None, self.image_width, self.image_height,1])\n",
    "        self.Y = tf.placeholder(\"float\", [None, self.num_classes])\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = 32\n",
    "    def add_conv(self,x,filters,shape,last_filter):\n",
    "        if(filters-last_filter != 0):\n",
    "            zero_padding = tf.zeros([self.batch_size,self.image_width/2, self.image_width/2,filters-last_filter], tf.float32)\n",
    "            x = tf.concat([x, zero_padding], 3)\n",
    "        conv1 = tf.layers.conv2d(\n",
    "          inputs=x,\n",
    "          filters=filters,\n",
    "          kernel_size=shape,\n",
    "          padding=\"same\",\n",
    "          activation=None)\n",
    "        conv2 = tf.layers.conv2d(\n",
    "          inputs=conv1,\n",
    "          filters=filters,\n",
    "          kernel_size=shape,\n",
    "          padding=\"same\",\n",
    "          activation=None)\n",
    "        new_x = x + conv1\n",
    "        return tf.nn.relu(new_x)\n",
    "    def build(self):\n",
    "        input_layer = tf.reshape(self.input_x, [-1, self.image_width, self.image_height, 1])\n",
    "        conv1 = tf.layers.conv2d(\n",
    "          inputs=input_layer,\n",
    "          filters=64,\n",
    "          kernel_size=[7,7],\n",
    "          padding=\"same\",\n",
    "          activation=None)\n",
    "        pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)\n",
    "        temp = pool1\n",
    "        last_filter = 64\n",
    "        for i in range(3):\n",
    "            temp = self.add_conv(temp,64,[3,3],last_filter)\n",
    "            last_filter = 64\n",
    "        for i in range(4):\n",
    "            temp = self.add_conv(temp,128,[3,3],last_filter)\n",
    "            last_filter = 128\n",
    "        for i in range(6):\n",
    "            temp = self.add_conv(temp,256,[3,3],last_filter)\n",
    "            last_filter = 256\n",
    "        for i in range(3):\n",
    "            temp = self.add_conv(temp,512,[3,3],last_filter)\n",
    "            last_filter = 512\n",
    "        avg_pool1 = tf.layers.average_pooling2d(inputs=temp, pool_size=[2, 2], strides=2)\n",
    "        flatten = tf.contrib.layers.flatten(\n",
    "          avg_pool1,\n",
    "          outputs_collections=None,\n",
    "          scope=None\n",
    "        )\n",
    "        num_classes = 10\n",
    "        weights = {\n",
    "            'w1': tf.Variable(tf.random_normal([tf.shape(flatten)[1], 1000])),\n",
    "            'w2': tf.Variable(tf.random_normal([1000, num_classes]))\n",
    "        }\n",
    "        biases = {\n",
    "            'b1': tf.Variable(tf.random_normal([1000])),\n",
    "            'b2': tf.Variable(tf.random_normal([num_classes]))\n",
    "        }\n",
    "        temp = tf.matmul(flatten, weights['w1']) + biases['b1']\n",
    "        logits = tf.matmul(temp, weights['w2']) + biases['b2']\n",
    "        prediction = tf.nn.softmax(logits)\n",
    "        \"\"\"\n",
    "        loss\n",
    "        \"\"\"\n",
    "        # Define loss and optimizer\n",
    "        self.loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "            logits=logits, labels=self.Y))\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate)\n",
    "        self.train_op = self.optimizer.minimize(self.loss_op)\n",
    "        \n",
    "        # Evaluate model (with test logits, for dropout to be disabled)\n",
    "        self.correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(self.Y, 1))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(self.correct_pred, tf.float32))\n",
    "    def train(self,mnist,training_steps,batch_size = 32):\n",
    "        # Initialize the variables (i.e. assign their default value)\n",
    "        init = tf.global_variables_initializer()\n",
    "        display_step = 100\n",
    "        # Start training\n",
    "        with tf.Session() as sess:\n",
    "\n",
    "            # Run the initializer\n",
    "            sess.run(init)\n",
    "\n",
    "            for step in range(1, training_steps+1):\n",
    "                batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "                # Reshape data to get 28 seq of 28 elements\n",
    "                batch_x = batch_x.reshape((batch_size, self.image_width, self.image_height,1))\n",
    "                # Run optimization op (backprop)\n",
    "                sess.run(self.train_op, feed_dict={self.input_x: batch_x, self.Y: batch_y})\n",
    "                if step % display_step == 0 or step == 1:\n",
    "                    # Calculate batch loss and accuracy\n",
    "                    loss, acc = sess.run([self.loss_op, self.accuracy], feed_dict={self.input_x: batch_x,\n",
    "                                                                         self.Y: batch_y})\n",
    "                    print(\"Step \" + str(step) + \", Minibatch Loss= \" + \\\n",
    "                          \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                          \"{:.3f}\".format(acc))\n",
    "\n",
    "            print(\"Optimization Finished!\")\n",
    "\n",
    "            # Calculate accuracy for 128 mnist test images\n",
    "            test_len = 128\n",
    "            test_data = mnist.test.images[:test_len].reshape((-1, image_width, image_height))\n",
    "            test_label = mnist.test.labels[:test_len]\n",
    "            print(\"Testing Accuracy:\", \n",
    "                sess.run(accuracy, feed_dict={X: test_data, Y: test_label}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-d44be5810480>:75: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "net = Residual_net([28,28])\n",
    "net.build()\n",
    "net.train(mnist,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[1,2] + [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
