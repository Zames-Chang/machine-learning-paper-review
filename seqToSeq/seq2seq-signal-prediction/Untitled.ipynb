{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mySeq2seq():\n",
    "    def __init__(self,X,Y,batch_size = 50,hidden_dim = 35,layers_stacked_count = 2,learning_rate = 0.007,lr_decay = 0.92,momentum = 0.5,lambda_l2_reg = 0.003):\n",
    "        seq_length = X[0]  # Time series will have the same past and future (to be predicted) lenght. \n",
    "        output_dim = Y[-1]\n",
    "        input_dim = X[-1]  # Output dimension (e.g.: multiple signals at once, tied in time)\n",
    "        try:\n",
    "            tf.nn.seq2seq = tf.contrib.legacy_seq2seq\n",
    "            tf.nn.rnn_cell = tf.contrib.rnn\n",
    "            tf.nn.rnn_cell.GRUCell = tf.contrib.rnn.GRUCell\n",
    "            print(\"TensorFlow's version : 1.0 (or more)\")\n",
    "        except: \n",
    "            print(\"TensorFlow's version : 0.12\")\n",
    "        tf.reset_default_graph()\n",
    "        # sess.close()\n",
    "        sess = tf.InteractiveSession()\n",
    "        with tf.variable_scope('Seq2seq'):\n",
    "            enc_inp = [\n",
    "                tf.placeholder(tf.float32, shape=(None, input_dim), name=\"inp_{}\".format(t))\n",
    "                   for t in range(seq_length)\n",
    "            ]\n",
    "            expected_sparse_output = [\n",
    "                tf.placeholder(tf.float32, shape=(None, output_dim), name=\"expected_sparse_output_\".format(t))\n",
    "                  for t in range(seq_length)\n",
    "            ]\n",
    "            dec_inp = [ tf.zeros_like(enc_inp[0], dtype=np.float32, name=\"GO\") ] + enc_inp[:-1]\n",
    "            cells = []\n",
    "            for i in range(layers_stacked_count):\n",
    "                with tf.variable_scope('RNN_{}'.format(i)):\n",
    "                    cells.append(tf.nn.rnn_cell.GRUCell(hidden_dim))\n",
    "                    # cells.append(tf.nn.rnn_cell.BasicLSTMCell(...))\n",
    "            cell = tf.nn.rnn_cell.MultiRNNCell(cells)\n",
    "            dec_outputs, dec_memory = tf.nn.seq2seq.basic_rnn_seq2seq(\n",
    "                enc_inp, \n",
    "                dec_inp, \n",
    "                cell\n",
    "            )\n",
    "            w_out = tf.Variable(tf.random_normal([hidden_dim, output_dim]))\n",
    "            b_out = tf.Variable(tf.random_normal([output_dim]))\n",
    "            output_scale_factor = tf.Variable(1.0, name=\"Output_ScaleFactor\")\n",
    "            reshaped_outputs = [output_scale_factor*(tf.matmul(i, w_out) + b_out) for i in dec_outputs]\n",
    "        with tf.variable_scope('Loss'):\n",
    "            # L2 loss\n",
    "            output_loss = 0\n",
    "            for _y, _Y in zip(reshaped_outputs, expected_sparse_output):\n",
    "                output_loss += tf.reduce_mean(tf.nn.l2_loss(_y - _Y))\n",
    "\n",
    "            # L2 regularization (to avoid overfitting and to have a  better generalization capacity)\n",
    "            reg_loss = 0\n",
    "            for tf_var in tf.trainable_variables():\n",
    "                if not (\"Bias\" in tf_var.name or \"Output_\" in tf_var.name):\n",
    "                    reg_loss += tf.reduce_mean(tf.nn.l2_loss(tf_var))\n",
    "\n",
    "            loss = output_loss + lambda_l2_reg * reg_loss\n",
    "\n",
    "        with tf.variable_scope('Optimizer'):\n",
    "            optimizer = tf.train.RMSPropOptimizer(learning_rate, decay=lr_decay, momentum=momentum)\n",
    "            train_op = optimizer.minimize(loss)\n",
    "    def train(batch_size,nb_iters,TrainX,TrainY,TestX,TestY):\n",
    "        train_losses = []\n",
    "        test_losses = []\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for t in range(nb_iters+1):\n",
    "            train_loss = train_batch(batch_size,TrainX,TrainY)\n",
    "            train_losses.append(train_loss)\n",
    "\n",
    "            if t % 10 == 0: \n",
    "                # Tester\n",
    "                test_loss = test_batch(batch_size,TestX,TestY)\n",
    "                test_losses.append(test_loss)\n",
    "                print(\"Step {}/{}, train loss: {}, \\tTEST loss: {}\".format(t, nb_iters, train_loss, test_loss))\n",
    "\n",
    "        print(\"Fin. train loss: {}, \\tTEST loss: {}\".format(train_loss, test_loss))\n",
    "    def train_batch(batch_size,X,Y):\n",
    "        \"\"\"\n",
    "        Training step that optimizes the weights \n",
    "        provided some batch_size X and Y examples from the dataset. \n",
    "        \"\"\"\n",
    "        feed_dict = {enc_inp[t]: X[t] for t in range(len(X))}\n",
    "        feed_dict.update({expected_sparse_output[t]: Y[t] for t in range(len(Y))})\n",
    "        _, loss_t = sess.run([train_op, loss], feed_dict)\n",
    "        return loss_t\n",
    "\n",
    "    def test_batch(batch_size):\n",
    "        \"\"\"\n",
    "        Test step, does NOT optimizes. Weights are frozen by not\n",
    "        doing sess.run on the train_op. \n",
    "        \"\"\"\n",
    "        feed_dict = {enc_inp[t]: X[t] for t in range(len(X))}\n",
    "        feed_dict.update({expected_sparse_output[t]: Y[t] for t in range(len(Y))})\n",
    "        loss_t = sess.run([loss], feed_dict)\n",
    "        return loss_t[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
